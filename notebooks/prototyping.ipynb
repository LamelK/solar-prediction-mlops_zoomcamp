{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82802aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "import mlflow.exceptions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef71880",
   "metadata": {},
   "source": [
    "## Data Loading and Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092eecf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNIXTime</th>\n",
       "      <th>Data</th>\n",
       "      <th>Time</th>\n",
       "      <th>Radiation</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>WindDirection(Degrees)</th>\n",
       "      <th>Speed</th>\n",
       "      <th>TimeSunRise</th>\n",
       "      <th>TimeSunSet</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1472724008</td>\n",
       "      <td>9/1/2016 12:00:00 AM</td>\n",
       "      <td>00:00:08</td>\n",
       "      <td>2.58</td>\n",
       "      <td>51</td>\n",
       "      <td>30.43</td>\n",
       "      <td>103</td>\n",
       "      <td>77.27</td>\n",
       "      <td>11.25</td>\n",
       "      <td>06:07:00</td>\n",
       "      <td>18:38:00</td>\n",
       "      <td>2016-09-01 10:00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1472724310</td>\n",
       "      <td>9/1/2016 12:00:00 AM</td>\n",
       "      <td>00:05:10</td>\n",
       "      <td>2.83</td>\n",
       "      <td>51</td>\n",
       "      <td>30.43</td>\n",
       "      <td>103</td>\n",
       "      <td>153.44</td>\n",
       "      <td>9.00</td>\n",
       "      <td>06:07:00</td>\n",
       "      <td>18:38:00</td>\n",
       "      <td>2016-09-01 10:05:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1472725206</td>\n",
       "      <td>9/1/2016 12:00:00 AM</td>\n",
       "      <td>00:20:06</td>\n",
       "      <td>2.16</td>\n",
       "      <td>51</td>\n",
       "      <td>30.43</td>\n",
       "      <td>103</td>\n",
       "      <td>142.04</td>\n",
       "      <td>7.87</td>\n",
       "      <td>06:07:00</td>\n",
       "      <td>18:38:00</td>\n",
       "      <td>2016-09-01 10:20:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1472725505</td>\n",
       "      <td>9/1/2016 12:00:00 AM</td>\n",
       "      <td>00:25:05</td>\n",
       "      <td>2.21</td>\n",
       "      <td>51</td>\n",
       "      <td>30.43</td>\n",
       "      <td>103</td>\n",
       "      <td>144.12</td>\n",
       "      <td>18.00</td>\n",
       "      <td>06:07:00</td>\n",
       "      <td>18:38:00</td>\n",
       "      <td>2016-09-01 10:25:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1472725809</td>\n",
       "      <td>9/1/2016 12:00:00 AM</td>\n",
       "      <td>00:30:09</td>\n",
       "      <td>2.25</td>\n",
       "      <td>51</td>\n",
       "      <td>30.43</td>\n",
       "      <td>103</td>\n",
       "      <td>67.42</td>\n",
       "      <td>11.25</td>\n",
       "      <td>06:07:00</td>\n",
       "      <td>18:38:00</td>\n",
       "      <td>2016-09-01 10:30:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     UNIXTime                  Data      Time  Radiation  Temperature  \\\n",
       "0  1472724008  9/1/2016 12:00:00 AM  00:00:08       2.58           51   \n",
       "1  1472724310  9/1/2016 12:00:00 AM  00:05:10       2.83           51   \n",
       "2  1472725206  9/1/2016 12:00:00 AM  00:20:06       2.16           51   \n",
       "3  1472725505  9/1/2016 12:00:00 AM  00:25:05       2.21           51   \n",
       "4  1472725809  9/1/2016 12:00:00 AM  00:30:09       2.25           51   \n",
       "\n",
       "   Pressure  Humidity  WindDirection(Degrees)  Speed TimeSunRise TimeSunSet  \\\n",
       "0     30.43       103                   77.27  11.25    06:07:00   18:38:00   \n",
       "1     30.43       103                  153.44   9.00    06:07:00   18:38:00   \n",
       "2     30.43       103                  142.04   7.87    06:07:00   18:38:00   \n",
       "3     30.43       103                  144.12  18.00    06:07:00   18:38:00   \n",
       "4     30.43       103                   67.42  11.25    06:07:00   18:38:00   \n",
       "\n",
       "              datetime  \n",
       "0  2016-09-01 10:00:08  \n",
       "1  2016-09-01 10:05:10  \n",
       "2  2016-09-01 10:20:06  \n",
       "3  2016-09-01 10:25:05  \n",
       "4  2016-09-01 10:30:09  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc06fd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26148 entries, 0 to 26147\n",
      "Data columns (total 12 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   UNIXTime                26148 non-null  int64  \n",
      " 1   Data                    26148 non-null  object \n",
      " 2   Time                    26148 non-null  object \n",
      " 3   Radiation               26148 non-null  float64\n",
      " 4   Temperature             26148 non-null  int64  \n",
      " 5   Pressure                26148 non-null  float64\n",
      " 6   Humidity                26148 non-null  int64  \n",
      " 7   WindDirection(Degrees)  26148 non-null  float64\n",
      " 8   Speed                   26148 non-null  float64\n",
      " 9   TimeSunRise             26148 non-null  object \n",
      " 10  TimeSunSet              26148 non-null  object \n",
      " 11  datetime                26148 non-null  object \n",
      "dtypes: float64(4), int64(3), object(5)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4da8c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNIXTime</th>\n",
       "      <th>Radiation</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>WindDirection(Degrees)</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.614800e+04</td>\n",
       "      <td>26148.000000</td>\n",
       "      <td>26148.000000</td>\n",
       "      <td>26148.000000</td>\n",
       "      <td>26148.000000</td>\n",
       "      <td>26148.000000</td>\n",
       "      <td>26148.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.476988e+09</td>\n",
       "      <td>218.000950</td>\n",
       "      <td>51.861749</td>\n",
       "      <td>30.434092</td>\n",
       "      <td>74.614349</td>\n",
       "      <td>134.404504</td>\n",
       "      <td>6.042688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.367042e+06</td>\n",
       "      <td>326.410173</td>\n",
       "      <td>6.177191</td>\n",
       "      <td>0.037621</td>\n",
       "      <td>26.249730</td>\n",
       "      <td>79.359142</td>\n",
       "      <td>3.049319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.472724e+09</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>30.280000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.474960e+09</td>\n",
       "      <td>1.230000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>30.410000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>74.307500</td>\n",
       "      <td>3.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.477042e+09</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>30.440000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>139.880000</td>\n",
       "      <td>5.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.479008e+09</td>\n",
       "      <td>380.567500</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>30.460000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>173.540000</td>\n",
       "      <td>7.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.481299e+09</td>\n",
       "      <td>1601.260000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>30.540000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>359.950000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           UNIXTime     Radiation   Temperature      Pressure      Humidity  \\\n",
       "count  2.614800e+04  26148.000000  26148.000000  26148.000000  26148.000000   \n",
       "mean   1.476988e+09    218.000950     51.861749     30.434092     74.614349   \n",
       "std    2.367042e+06    326.410173      6.177191      0.037621     26.249730   \n",
       "min    1.472724e+09      1.130000     38.000000     30.280000      8.000000   \n",
       "25%    1.474960e+09      1.230000     47.000000     30.410000     55.000000   \n",
       "50%    1.477042e+09      2.900000     51.000000     30.440000     85.000000   \n",
       "75%    1.479008e+09    380.567500     56.000000     30.460000     97.000000   \n",
       "max    1.481299e+09   1601.260000     71.000000     30.540000    103.000000   \n",
       "\n",
       "       WindDirection(Degrees)         Speed  \n",
       "count            26148.000000  26148.000000  \n",
       "mean               134.404504      6.042688  \n",
       "std                 79.359142      3.049319  \n",
       "min                  0.090000      0.000000  \n",
       "25%                 74.307500      3.370000  \n",
       "50%                139.880000      5.620000  \n",
       "75%                173.540000      7.870000  \n",
       "max                359.950000     27.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic statistics of the DataFrame\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dbb387",
   "metadata": {},
   "source": [
    "### Data Cleanng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e8a97a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check duplicate count\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "841d7f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIXTime                  0\n",
      "Data                      0\n",
      "Time                      0\n",
      "Radiation                 0\n",
      "Temperature               0\n",
      "Pressure                  0\n",
      "Humidity                  0\n",
      "WindDirection(Degrees)    0\n",
      "Speed                     0\n",
      "TimeSunRise               0\n",
      "TimeSunSet                0\n",
      "datetime                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b42ab1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates \n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64f6b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with all nulls \n",
    "df.dropna(how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a4f15d",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc68484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26148, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the DataFrame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fdb389",
   "metadata": {},
   "source": [
    "##### cyclical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4b7c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert UNIXTime to datetime\n",
    "df['DateTime'] = pd.to_datetime(df['UNIXTime'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1324509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract datetime features\n",
    "df['Hour'] = df['DateTime'].dt.hour\n",
    "df['Minute'] = df['DateTime'].dt.minute\n",
    "df['Day'] = df['DateTime'].dt.day\n",
    "df['Month'] = df['DateTime'].dt.month\n",
    "df['Weekday'] = df['DateTime'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd095b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hour of day (0‚Äì23)\n",
    "df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)\n",
    "df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)\n",
    "\n",
    "# Minute (0‚Äì59)\n",
    "df['Minute_sin'] = np.sin(2 * np.pi * df['Minute'] / 60)\n",
    "df['Minute_cos'] = np.cos(2 * np.pi * df['Minute'] / 60)\n",
    "\n",
    "# Day of month (1‚Äì31)\n",
    "df['Day_sin'] = np.sin(2 * np.pi * df['Day'] / 31)\n",
    "df['Day_cos'] = np.cos(2 * np.pi * df['Day'] / 31)\n",
    "\n",
    "# Month (1‚Äì12)\n",
    "df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "\n",
    "# Weekday (0=Monday)\n",
    "df['Weekday_sin'] = np.sin(2 * np.pi * df['Weekday'] / 7)\n",
    "df['Weekday_cos'] = np.cos(2 * np.pi * df['Weekday'] / 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf3ef0f",
   "metadata": {},
   "source": [
    "#### Process Sunrise/Sunset Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "350ad2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sunrise/sunset to timedelta\n",
    "df['TimeSunRise_obj'] = pd.to_timedelta(df['TimeSunRise'])\n",
    "df['TimeSunSet_obj'] = pd.to_timedelta(df['TimeSunSet'])\n",
    "\n",
    "# Create actual sunrise/sunset datetime based on date of DateTime\n",
    "df['SunriseDateTime'] = df['DateTime'].dt.normalize() + df['TimeSunRise_obj']\n",
    "df['SunsetDateTime'] = df['DateTime'].dt.normalize() + df['TimeSunSet_obj']\n",
    "\n",
    "# Calculate minutes since sunrise / until sunset\n",
    "df['MinutesSinceSunrise'] = (df['DateTime'] - df['SunriseDateTime']).dt.total_seconds() / 60\n",
    "df['MinutesUntilSunset'] = (df['SunsetDateTime'] - df['DateTime']).dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f3e3b",
   "metadata": {},
   "source": [
    "##### Drop Columns that are not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfe61ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\n",
    "    'UNIXTime', 'Data', 'Time',\n",
    "    'TimeSunRise', 'TimeSunSet',\n",
    "    'TimeSunRise_obj', 'TimeSunSet_obj',\n",
    "    'Hour', 'Minute', 'Day', \n",
    "    'Month', 'Weekday', 'DateTime'\n",
    "], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047c8aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26148, 21)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape after dropping columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "872d833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26148 entries, 0 to 26147\n",
      "Data columns (total 21 columns):\n",
      " #   Column                  Non-Null Count  Dtype         \n",
      "---  ------                  --------------  -----         \n",
      " 0   Radiation               26148 non-null  float64       \n",
      " 1   Temperature             26148 non-null  int64         \n",
      " 2   Pressure                26148 non-null  float64       \n",
      " 3   Humidity                26148 non-null  int64         \n",
      " 4   WindDirection(Degrees)  26148 non-null  float64       \n",
      " 5   Speed                   26148 non-null  float64       \n",
      " 6   datetime                26148 non-null  object        \n",
      " 7   Hour_sin                26148 non-null  float64       \n",
      " 8   Hour_cos                26148 non-null  float64       \n",
      " 9   Minute_sin              26148 non-null  float64       \n",
      " 10  Minute_cos              26148 non-null  float64       \n",
      " 11  Day_sin                 26148 non-null  float64       \n",
      " 12  Day_cos                 26148 non-null  float64       \n",
      " 13  Month_sin               26148 non-null  float64       \n",
      " 14  Month_cos               26148 non-null  float64       \n",
      " 15  Weekday_sin             26148 non-null  float64       \n",
      " 16  Weekday_cos             26148 non-null  float64       \n",
      " 17  SunriseDateTime         26148 non-null  datetime64[ns]\n",
      " 18  SunsetDateTime          26148 non-null  datetime64[ns]\n",
      " 19  MinutesSinceSunrise     26148 non-null  float64       \n",
      " 20  MinutesUntilSunset      26148 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(16), int64(2), object(1)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82f04463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Month_sin                     4\n",
       "Month_cos                     4\n",
       "Weekday_sin                   7\n",
       "Weekday_cos                   7\n",
       "Hour_cos                     22\n",
       "Hour_sin                     22\n",
       "Speed                        24\n",
       "Minute_cos                   26\n",
       "Day_cos                      26\n",
       "Minute_sin                   26\n",
       "Pressure                     27\n",
       "Day_sin                      31\n",
       "Temperature                  34\n",
       "Humidity                     94\n",
       "SunriseDateTime             136\n",
       "SunsetDateTime              154\n",
       "Radiation                 11994\n",
       "WindDirection(Degrees)    15178\n",
       "MinutesUntilSunset        16092\n",
       "MinutesSinceSunrise       16096\n",
       "datetime                  26148\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9409240",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b141aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Radiation', 'Temperature', 'Pressure', 'Humidity',\n",
       "       'WindDirection(Degrees)', 'Speed', 'datetime', 'Hour_sin', 'Hour_cos',\n",
       "       'Minute_sin', 'Minute_cos', 'Day_sin', 'Day_cos', 'Month_sin',\n",
       "       'Month_cos', 'Weekday_sin', 'Weekday_cos', 'SunriseDateTime',\n",
       "       'SunsetDateTime', 'MinutesSinceSunrise', 'MinutesUntilSunset'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624e2e74",
   "metadata": {},
   "source": [
    "#### Base Models and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f291a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Temperature', 'Pressure', 'Humidity',\n",
    "       'WindDirection(Degrees)', 'Speed', 'Hour_sin', \n",
    "       'Hour_cos', 'Minute_sin', 'Minute_cos', 'Day_sin', \n",
    "       'Day_cos', 'Month_sin', 'Month_cos', 'Weekday_sin', \n",
    "       'Weekday_cos', 'MinutesSinceSunrise', 'MinutesUntilSunset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ed2ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RandomForest with all features...\n",
      "Selected features for RandomForest: ['Temperature', 'Pressure', 'Humidity', 'WindDirection(Degrees)', 'Speed', 'Day_sin', 'Day_cos', 'Month_sin', 'MinutesSinceSunrise', 'MinutesUntilSunset']\n",
      "RandomForest Base Model -> RMSE: 76.5182, R2: 0.9454\n",
      "\n",
      "Training XGBoost with all features...\n",
      "Selected features for XGBoost: ['Temperature', 'Pressure', 'Humidity', 'WindDirection(Degrees)', 'Speed', 'Hour_sin', 'Hour_cos', 'Minute_sin', 'Minute_cos', 'Day_sin', 'Day_cos', 'Month_sin', 'Weekday_sin', 'Weekday_cos', 'MinutesSinceSunrise', 'MinutesUntilSunset']\n",
      "XGBoost Base Model -> RMSE: 80.3027, R2: 0.9399\n",
      "\n",
      "Training CatBoost with all features...\n",
      "Selected features for CatBoost: ['Temperature', 'Pressure', 'Humidity', 'WindDirection(Degrees)', 'Speed', 'Hour_sin', 'Hour_cos', 'Minute_sin', 'Minute_cos', 'Day_sin', 'Day_cos', 'Month_sin', 'Month_cos', 'Weekday_sin', 'Weekday_cos', 'MinutesSinceSunrise', 'MinutesUntilSunset']\n",
      "CatBoost Base Model -> RMSE: 77.9769, R2: 0.9433\n",
      "\n",
      "Selected Features Per Model:\n",
      " {'RandomForest': ['Temperature', 'Pressure', 'Humidity', 'WindDirection(Degrees)', 'Speed', 'Day_sin', 'Day_cos', 'Month_sin', 'MinutesSinceSunrise', 'MinutesUntilSunset'], 'XGBoost': ['Temperature', 'Pressure', 'Humidity', 'WindDirection(Degrees)', 'Speed', 'Hour_sin', 'Hour_cos', 'Minute_sin', 'Minute_cos', 'Day_sin', 'Day_cos', 'Month_sin', 'Weekday_sin', 'Weekday_cos', 'MinutesSinceSunrise', 'MinutesUntilSunset'], 'CatBoost': ['Temperature', 'Pressure', 'Humidity', 'WindDirection(Degrees)', 'Speed', 'Hour_sin', 'Hour_cos', 'Minute_sin', 'Minute_cos', 'Day_sin', 'Day_cos', 'Month_sin', 'Month_cos', 'Weekday_sin', 'Weekday_cos', 'MinutesSinceSunrise', 'MinutesUntilSunset']}\n",
      "\n",
      "Base Model Metrics:\n",
      " {'RandomForest': {'RMSE': np.float64(76.51822160081764), 'R2': 0.9454394428955204}, 'XGBoost': {'RMSE': np.float64(80.30268289158863), 'R2': 0.9399090360384554}, 'CatBoost': {'RMSE': np.float64(77.97693057163666), 'R2': 0.9433393786991401}}\n"
     ]
    }
   ],
   "source": [
    "# Sample Data\n",
    "X = df[features]\n",
    "y = df['Radiation']\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'XGBoost': XGBRegressor(),\n",
    "    'CatBoost': CatBoostRegressor(verbose=0)\n",
    "}\n",
    "\n",
    "# Feature selection threshold\n",
    "feature_threshold = 0.005 \n",
    "selected_features_per_model = {}  # Dictionary to store selected features per model\n",
    "metrics_per_model = {} # Dictionary to store metrics per model\n",
    "\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name} with all features...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Extract feature importances\n",
    "    if name == 'XGBoost':\n",
    "        importances = pd.Series(model.get_booster().get_score(importance_type='weight'))\n",
    "        importances = importances / importances.sum()\n",
    "        importances = importances.reindex(X.columns, fill_value=0)\n",
    "    else:\n",
    "        importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "    \n",
    "    selected_features = importances[importances >= feature_threshold].index.tolist()\n",
    "    selected_features_per_model[name] = selected_features\n",
    "    print(f\"Selected features for {name}: {selected_features}\")\n",
    "    \n",
    "    # Evaluate base model\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    metrics_per_model[name] = {'RMSE': rmse, 'R2': r2}\n",
    "    print(f\"{name} Base Model -> RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "# Final dictionaries to reuse\n",
    "print(\"\\nSelected Features Per Model:\\n\", selected_features_per_model)\n",
    "print(\"\\nBase Model Metrics:\\n\", metrics_per_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43138820",
   "metadata": {},
   "source": [
    "#### Tuning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f053ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Val / Test Split\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.15, random_state=42)\n",
    "\n",
    "# Define parameter grids for tuning\n",
    "param_grids = {\n",
    "    'RandomForest': [\n",
    "        {}, \n",
    "        {'n_estimators': 150, 'max_depth': None},\n",
    "        {'n_estimators': 300, 'max_depth': 15},\n",
    "    ],\n",
    "    'XGBoost': [\n",
    "        {},\n",
    "        {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.05},\n",
    "        {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.03},\n",
    "    ],\n",
    "    'CatBoost': [\n",
    "        {}, \n",
    "        {'iterations': 200, 'depth': 5, 'learning_rate': 0.05, 'verbose': 0},\n",
    "        {'iterations': 300, 'depth': 6, 'learning_rate': 0.03, 'verbose': 0},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Initialize a list to store all runs\n",
    "all_runs = []\n",
    "\n",
    "# Iterate through each model and its parameter grid\n",
    "for model_name, param_list in param_grids.items():\n",
    "    features = selected_features_per_model[model_name]\n",
    "\n",
    "    print(f\"\\nTuning {model_name}...\")\n",
    "\n",
    "    # Iterate through each set of parameters\n",
    "    for i, params in enumerate(param_list, 1):\n",
    "        if model_name == 'RandomForest':\n",
    "            model = RandomForestRegressor(**params)\n",
    "        elif model_name == 'XGBoost':\n",
    "            model = XGBRegressor(**params)\n",
    "        elif model_name == 'CatBoost':\n",
    "            params['verbose'] = 0\n",
    "            model = CatBoostRegressor(**params)\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train[features], y_train)\n",
    "        # Validate the model\n",
    "        val_preds = model.predict(X_val[features])\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "        val_r2 = r2_score(y_val, val_preds)\n",
    "\n",
    "        # Store the run details\n",
    "        all_runs.append({\n",
    "            'model_name': model_name,\n",
    "            'params': params,\n",
    "            'features': features,\n",
    "            'val_rmse': val_rmse,\n",
    "            'val_r2': val_r2,\n",
    "            'model': model\n",
    "        })\n",
    "\n",
    "        print(f\"Run {i} | Params: {params} | Val RMSE: {val_rmse:.4f}, Val R2: {val_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nTotal runs completed: {len(all_runs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73974c65",
   "metadata": {},
   "source": [
    "#### Log to MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1200c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/14 12:59:50 INFO mlflow.tracking.fluent: Experiment with name 'My_Model_Experiment' does not exist. Creating a new experiment.\n",
      "/home/lamel/anaconda3/envs/mlops_env/lib/python3.11/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run RandomForest_run_1 at: http://localhost:5000/#/experiments/1/runs/d3314c244bf64e4389f8516c2df9f69e\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamel/anaconda3/envs/mlops_env/lib/python3.11/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run RandomForest_run_2 at: http://localhost:5000/#/experiments/1/runs/fd828a3cde2b48e19a719dc43578f35e\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamel/anaconda3/envs/mlops_env/lib/python3.11/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run RandomForest_run_3 at: http://localhost:5000/#/experiments/1/runs/67a8c9761ceb4ce297fe49264c15029d\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamel/anaconda3/envs/mlops_env/lib/python3.11/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run XGBoost_run_4 at: http://localhost:5000/#/experiments/1/runs/bd8c4cd4221c45daac70e996c44c2735\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamel/anaconda3/envs/mlops_env/lib/python3.11/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run XGBoost_run_5 at: http://localhost:5000/#/experiments/1/runs/ebbf444252d3403787a8e24ffcc35cd6\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamel/anaconda3/envs/mlops_env/lib/python3.11/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run XGBoost_run_6 at: http://localhost:5000/#/experiments/1/runs/de2e7e0998ab429eb0b40382b402f941\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamel/anaconda3/envs/mlops_env/lib/python3.11/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run CatBoost_run_7 at: http://localhost:5000/#/experiments/1/runs/19e0b7023c034338a542ef94e9502abb\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamel/anaconda3/envs/mlops_env/lib/python3.11/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run CatBoost_run_8 at: http://localhost:5000/#/experiments/1/runs/feb4f2e67e6d4d6e99f9ac1b6c18d2aa\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamel/anaconda3/envs/mlops_env/lib/python3.11/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run CatBoost_run_9 at: http://localhost:5000/#/experiments/1/runs/e65d168526824ae8899a0137a17a55a1\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n",
      "Logged 9 runs to MLflow.\n"
     ]
    }
   ],
   "source": [
    "# set up MLflow tracking and experiment\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"My_Model_Experiment\")\n",
    "\n",
    "# Initialize a list to store logged runs\n",
    "logged_runs = []\n",
    "\n",
    "print(\"Starting test evaluation...\\n\")\n",
    "\n",
    "# Iterate through all runs and log to MLflow\n",
    "for idx, run in enumerate(all_runs, 1):\n",
    "    try:\n",
    "        with mlflow.start_run(run_name=f\"{run['model_name']}_run_{idx}\") as mlflow_run:\n",
    "            # Log parameters\n",
    "            mlflow.log_params(run['params'])\n",
    "\n",
    "            # Log validation metrics\n",
    "            mlflow.log_metrics({'val_rmse': run['val_rmse'], 'val_r2': run['val_r2']})\n",
    "\n",
    "            # Infer signature from validation features and predictions\n",
    "            input_data = X_val[run['features']]\n",
    "            predictions = run['model'].predict(input_data)\n",
    "            signature = infer_signature(input_data, predictions)\n",
    "\n",
    "            # Log model with signature and example input\n",
    "            mlflow.sklearn.log_model(\n",
    "                sk_model=run['model'],\n",
    "                name=\"model\",\n",
    "                signature=signature,\n",
    "                input_example=input_data.iloc[:5]\n",
    "            )\n",
    "\n",
    "            # Track run info\n",
    "            logged_runs.append({\n",
    "                'run_id': mlflow_run.info.run_id,\n",
    "                **run\n",
    "            })\n",
    "    except mlflow.exceptions.MlflowException as e:\n",
    "        print(f\"Failed to log run {idx} for model {run['model_name']}: {e}\")\n",
    "\n",
    "print(f\"Logged {len(logged_runs)} runs to MLflow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2b5c5bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Models by Val RMSE:\n",
      "1. Model: RandomForest, Val RMSE: 82.0335\n",
      "2. Model: RandomForest, Val RMSE: 82.0494\n",
      "3. Model: RandomForest, Val RMSE: 82.2011\n"
     ]
    }
   ],
   "source": [
    "# Sort the logged runs by 'val_rmse' ascending (lower is better)\n",
    "top_3_runs = sorted(logged_runs, key=lambda x: x['val_rmse'])[:3]\n",
    "\n",
    "print(\"Top 3 Models by Val RMSE:\")\n",
    "for i, run in enumerate(top_3_runs, 1):\n",
    "    print(f\"{i}. Model: {run['model_name']}, Val RMSE: {run['val_rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a40906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test evaluation...\n",
      "\n",
      "üèÉ View run RandomForest_run_2 at: http://localhost:5000/#/experiments/1/runs/fd828a3cde2b48e19a719dc43578f35e\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n",
      "‚úÖ Run fd828a3cde2b48e19a719dc43578f35e | Model: RandomForest | Test RMSE: 75.6833, Test R¬≤: 0.9468\n",
      "üèÉ View run RandomForest_run_3 at: http://localhost:5000/#/experiments/1/runs/67a8c9761ceb4ce297fe49264c15029d\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n",
      "‚úÖ Run 67a8c9761ceb4ce297fe49264c15029d | Model: RandomForest | Test RMSE: 75.6127, Test R¬≤: 0.9469\n",
      "üèÉ View run RandomForest_run_1 at: http://localhost:5000/#/experiments/1/runs/d3314c244bf64e4389f8516c2df9f69e\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n",
      "‚úÖ Run d3314c244bf64e4389f8516c2df9f69e | Model: RandomForest | Test RMSE: 76.9956, Test R¬≤: 0.9450\n",
      "\n",
      "Sorted Test Results:\n",
      "Model: RandomForest, Test RMSE: 75.6127, Test R¬≤: 0.9469\n",
      "Model: RandomForest, Test RMSE: 75.6833, Test R¬≤: 0.9468\n",
      "Model: RandomForest, Test RMSE: 76.9956, Test R¬≤: 0.9450\n"
     ]
    }
   ],
   "source": [
    "# Load the MLflow client\n",
    "client = MlflowClient()\n",
    "# Prepare to evaluate the top 3 runs on the test set\n",
    "test_results = []\n",
    "\n",
    "print(\"Starting test evaluation...\\n\")\n",
    "\n",
    "# Iterate through the top 3 runs and evaluate on the test set\n",
    "for run in top_3_runs:\n",
    "    run_id = run['run_id']\n",
    "    model_name = run['model_name']\n",
    "    features = run['features']\n",
    "\n",
    "    # Load model from MLflow\n",
    "    model_uri = f\"runs:/{run_id}/model\"\n",
    "    model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "    # Predict on test data using training-time features\n",
    "    X_test_subset = X_test[features]\n",
    "    predictions = model.predict(X_test_subset)\n",
    "\n",
    "    # Calculate metrics\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    test_r2 = r2_score(y_test, predictions)\n",
    "\n",
    "    # Save result locally\n",
    "    test_results.append({\n",
    "        'run_id': run_id,\n",
    "        'model_name': model_name,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_r2': test_r2\n",
    "    })\n",
    "\n",
    "    # üöÄ Log metrics to existing MLflow run\n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        mlflow.log_metrics({\n",
    "            \"test_rmse\": test_rmse,\n",
    "            \"test_r2\": test_r2\n",
    "        })\n",
    "        mlflow.set_tag(\"evaluation_status\", \"tested_with_separate_script\")\n",
    "\n",
    "    print(f\"‚úÖ Run {run_id} | Model: {model_name} | Test RMSE: {test_rmse:.4f}, Test R¬≤: {test_r2:.4f}\")\n",
    "\n",
    "# Sort and display final results\n",
    "test_results_sorted = sorted(test_results, key=lambda x: x['test_rmse'])\n",
    "\n",
    "print(\"\\nSorted Test Results:\")\n",
    "for res in test_results_sorted:\n",
    "    print(f\"Model: {res['model_name']}, Test RMSE: {res['test_rmse']:.4f}, Test R¬≤: {res['test_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e7340f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'MyTopModel' already exists. Creating a new version of this model...\n",
      "2025/07/14 15:36:02 WARNING mlflow.tracking._model_registry.fluent: Run with id 67a8c9761ceb4ce297fe49264c15029d has no artifacts at artifact path 'model', registering model based on models:/m-5d6bc0b6a808442b8b68f22be593c46e instead\n",
      "2025/07/14 15:36:03 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: MyTopModel, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model registered: 'MyTopModel', Version: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'MyTopModel'.\n"
     ]
    }
   ],
   "source": [
    "# Register the best model based on validation RMSE\n",
    "client = MlflowClient()\n",
    "registry_model_name = \"MyTopModel\"\n",
    "model_uri = f\"runs:/{best_run_id}/model\"\n",
    "\n",
    "# Register the model\n",
    "registered_model = mlflow.register_model(model_uri, registry_model_name)\n",
    "\n",
    "print(f\"\\n‚úÖ Model registered: '{registry_model_name}', Version: {registered_model.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4264ca8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added metadata tags to the registered model\n"
     ]
    }
   ],
   "source": [
    "# Add metadata tags to the registered model\n",
    "client = MlflowClient()\n",
    "registry_model_name = \"MyTopModel\"\n",
    "\n",
    "# Add tags to registered model\n",
    "client.set_registered_model_tag(\n",
    "    name=registry_model_name,\n",
    "    key=\"model_type\",\n",
    "    value=best_run['model_name']\n",
    ")\n",
    "\n",
    "client.set_registered_model_tag(\n",
    "    name=registry_model_name,\n",
    "    key=\"test_rmse\",\n",
    "    value=str(best_run['test_rmse'])\n",
    ")\n",
    "\n",
    "client.set_registered_model_tag(\n",
    "    name=registry_model_name,\n",
    "    key=\"features_used\",\n",
    "    value=\",\".join(best_run['features'])  # Convert list to string\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Added metadata tags to the registered model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba3576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an alias for the registered model\n",
    "version = registered_model.version\n",
    "client.set_registered_model_alias(registry_model_name, \"champion\", version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e38703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the champion model from the registry\n",
    "model = mlflow.sklearn.load_model(f\"models:/{registry_model_name}@champion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ed32ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef5fb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GitRepository in module prefect.runner.storage:\n",
      "\n",
      "class GitRepository(builtins.object)\n",
      " |  GitRepository(url: 'str', credentials: 'Union[GitCredentials, Block, dict[str, Any], None]' = None, name: 'str | None' = None, branch: 'str | None' = None, commit_sha: 'str | None' = None, include_submodules: 'bool' = False, pull_interval: 'int | None' = 60, directories: 'list[str] | None' = None)\n",
      " |  \n",
      " |  Pulls the contents of a git repository to the local filesystem.\n",
      " |  \n",
      " |  Parameters:\n",
      " |      url: The URL of the git repository to pull from\n",
      " |      credentials: A dictionary of credentials to use when pulling from the\n",
      " |          repository. If a username is provided, an access token must also be\n",
      " |          provided.\n",
      " |      name: The name of the repository. If not provided, the name will be\n",
      " |          inferred from the repository URL.\n",
      " |      branch: The branch to pull from. Defaults to \"main\".\n",
      " |      pull_interval: The interval in seconds at which to pull contents from\n",
      " |          remote storage to local storage. If None, remote storage will perform\n",
      " |          a one-time sync.\n",
      " |      directories: The directories to pull from the Git repository (uses git sparse-checkout)\n",
      " |  \n",
      " |  Examples:\n",
      " |      Pull the contents of a private git repository to the local filesystem:\n",
      " |  \n",
      " |      ```python\n",
      " |      from prefect.runner.storage import GitRepository\n",
      " |  \n",
      " |      storage = GitRepository(\n",
      " |          url=\"https://github.com/org/repo.git\",\n",
      " |          credentials={\"username\": \"oauth2\", \"access_token\": \"my-access-token\"},\n",
      " |      )\n",
      " |  \n",
      " |      await storage.pull_code()\n",
      " |      ```\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, _GitRepository__value: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __init__(self, url: 'str', credentials: 'Union[GitCredentials, Block, dict[str, Any], None]' = None, name: 'str | None' = None, branch: 'str | None' = None, commit_sha: 'str | None' = None, include_submodules: 'bool' = False, pull_interval: 'int | None' = 60, directories: 'list[str] | None' = None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  async is_current_commit(self) -> 'bool'\n",
      " |      Check if the current commit is the same as the commit SHA\n",
      " |  \n",
      " |  async is_shallow_clone(self) -> 'bool'\n",
      " |      Check if the repository is a shallow clone\n",
      " |  \n",
      " |  async is_sparsely_checked_out(self) -> 'bool'\n",
      " |      Check if existing repo is sparsely checked out\n",
      " |  \n",
      " |  async pull_code(self) -> 'None'\n",
      " |      Pulls the contents of the configured repository to the local filesystem.\n",
      " |  \n",
      " |  set_base_path(self, path: 'Path') -> 'None'\n",
      " |  \n",
      " |  to_pull_step(self) -> 'dict[str, Any]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  destination\n",
      " |  \n",
      " |  pull_interval\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from prefect.runner.storage import GitRepository\n",
    "help(GitRepository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "136366db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class WorkPoolUpdate in module prefect.client.schemas.actions:\n",
      "\n",
      "class WorkPoolUpdate(prefect._internal.schemas.bases.ActionBaseModel)\n",
      " |  WorkPoolUpdate(**data: 'Any') -> 'None'\n",
      " |  \n",
      " |  Data used by the Prefect REST API to update a work pool.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      WorkPoolUpdate\n",
      " |      prefect._internal.schemas.bases.ActionBaseModel\n",
      " |      prefect._internal.schemas.bases.PrefectBaseModel\n",
      " |      pydantic.main.BaseModel\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'base_job_template': 'Optional[dict[str, Any]]', 'c...\n",
      " |  \n",
      " |  __class_vars__ = {'_reset_fields'}\n",
      " |  \n",
      " |  __private_attributes__ = {}\n",
      " |  \n",
      " |  __pydantic_complete__ = True\n",
      " |  \n",
      " |  __pydantic_computed_fields__ = {}\n",
      " |  \n",
      " |  __pydantic_core_schema__ = {'cls': <class 'prefect.client.schemas.acti...\n",
      " |  \n",
      " |  __pydantic_custom_init__ = False\n",
      " |  \n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |  \n",
      " |  __pydantic_fields__ = {'base_job_template': FieldInfo(annotation=Union...\n",
      " |  \n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |  \n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |  \n",
      " |  __pydantic_post_init__ = None\n",
      " |  \n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |  \n",
      " |  __pydantic_setattr_handlers__ = {}\n",
      " |  \n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"WorkPoolUpdate\", valid...\n",
      " |  \n",
      " |  model_config = {'defer_build': True, 'extra': 'forbid', 'ser_json_time...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from prefect._internal.schemas.bases.PrefectBaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |      Equality operator that ignores the resettable fields of the PrefectBaseModel.\n",
      " |      \n",
      " |      NOTE: this equality operator will only be applied if the PrefectBaseModel is\n",
      " |      the left-hand operand. This is a limitation of Python.\n",
      " |  \n",
      " |  __rich_repr__(self) -> Iterable[Union[Any, Tuple[Any], Tuple[str, Any], Tuple[str, Any, Any]]]\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |  \n",
      " |  reset_fields(self: Self) -> Self\n",
      " |      Reset the fields of the model that are in the `_reset_fields` set.\n",
      " |      \n",
      " |      Returns:\n",
      " |          PrefectBaseModel: A new instance of the model with the reset fields.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from prefect._internal.schemas.bases.PrefectBaseModel:\n",
      " |  \n",
      " |  model_validate_list(obj: Any, *, strict: Optional[bool] = None, from_attributes: Optional[bool] = None, context: Optional[Any] = None) -> list[typing.Self]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from prefect._internal.schemas.bases.PrefectBaseModel:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from prefect._internal.schemas.bases.PrefectBaseModel:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |  \n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |  \n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |  \n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __init__(self, /, **data: 'Any') -> 'None'\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      " |      validated to form a valid model.\n",
      " |      \n",
      " |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |  \n",
      " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]' from pydantic._internal._repr.Representation\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |  \n",
      " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      " |  \n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      " |  \n",
      " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_recursion__(self, object: 'Any') -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Returns the string representation of a recursive object.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
      " |  \n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |  \n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |      \n",
      " |      If you need `include` or `exclude`, use:\n",
      " |      \n",
      " |      ```python {test=\"skip\" lint=\"skip\"}\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |  \n",
      " |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      " |  \n",
      " |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_copy`](../concepts/serialization.md#model_copy)\n",
      " |      \n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      !!! note\n",
      " |          The underlying instance's [`__dict__`][object.__dict__] attribute is copied. This\n",
      " |          might have unexpected side effects if you store anything in it, on top of the model\n",
      " |          fields (e.g. the value of [cached properties][functools.cached_property]).\n",
      " |      \n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |  \n",
      " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump`](../concepts/serialization.md#modelmodel_dump)\n",
      " |      \n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |  \n",
      " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'str'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump_json`](../concepts/serialization.md#modelmodel_dump_json)\n",
      " |      \n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |      \n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |  \n",
      " |  model_post_init(self, context: 'Any', /) -> 'None'\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
      " |  \n",
      " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema'\n",
      " |  \n",
      " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue'\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |      \n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |  \n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called.\n",
      " |      \n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |      \n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by pydantic.\n",
      " |  \n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |  \n",
      " |  from_orm(obj: 'Any') -> 'Self'\n",
      " |  \n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |      \n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      \n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |      \n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |  \n",
      " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]'\n",
      " |      Generates a JSON schema for a model class.\n",
      " |      \n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |  \n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |      \n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |      \n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |  \n",
      " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None'\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |      \n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |      \n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |  \n",
      " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
      " |      Validate a pydantic model instance.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |  \n",
      " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [JSON Parsing](../concepts/json.md#json-parsing)\n",
      " |      \n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |  \n",
      " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |  \n",
      " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |  \n",
      " |  parse_obj(obj: 'Any') -> 'Self'\n",
      " |  \n",
      " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |  \n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
      " |  \n",
      " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
      " |  \n",
      " |  validate(value: 'Any') -> 'Self'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |  \n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __pydantic_extra__\n",
      " |  \n",
      " |  __pydantic_fields_set__\n",
      " |  \n",
      " |  __pydantic_private__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __pydantic_root_model__ = False\n",
      " |  \n",
      " |  model_computed_fields = {}\n",
      " |  \n",
      " |  model_fields = {'base_job_template': FieldInfo(annotation=Union[dict[s...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from prefect.client.schemas.actions import WorkPoolUpdate\n",
    "\n",
    "help(WorkPoolUpdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a41a4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
